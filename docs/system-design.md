# bft-maker — System Design

## Stack

Vanilla TypeScript. No framework. No runtime dependencies beyond Node. Output is numbered SQL files and a shell runner. DuckDB is the primary SQL dialect. Spark SQL is the secondary dialect for scale-out.

---

## Project Structure

```
bft-maker/
├── src/
│   ├── manifest/
│   │   ├── types.ts              # Manifest schema (TypeScript interfaces)
│   │   ├── validate.ts           # Manifest validation and consistency checks
│   │   └── estimate.ts           # Row count and cost estimators
│   │
│   ├── codegen/
│   │   ├── planner.ts            # Manifest → ordered build plan
│   │   ├── sql/
│   │   │   ├── allocation.ts     # Allocation strategy template
│   │   │   ├── elimination.ts    # Elimination strategy template
│   │   │   ├── reserve.ts        # Reserve strategy template
│   │   │   ├── sum-over-sum.ts   # Sum/Sum strategy template
│   │   │   ├── join.ts           # Final join assembly
│   │   │   └── validation.ts     # Test query generator
│   │   ├── emit.ts               # Writes numbered .sql files + run.sh
│   │   └── dialects/
│   │       ├── duckdb.ts         # DuckDB dialect (primary)
│   │       └── spark.ts          # Spark SQL dialect (scale-out)
│   │
│   └── cli/
│       └── index.ts              # CLI entry point
│
├── test/
│   ├── manifest/                 # Unit tests for validation and estimation
│   ├── codegen/                  # Snapshot tests: manifest in → SQL out
│   └── fixtures/                 # Sample manifests + synthetic data
│
├── data/                         # Test datasets (downloaded, not in git)
│   ├── northwind/
│   └── movielens/
│
├── docs/
│   ├── spec.md                   # What bft-maker does and how manifests work
│   └── system-design.md          # This file
│
├── dist/                         # Compiled output
├── tsconfig.json
└── package.json                  # devDependencies only: typescript
```

---

## Module Boundaries

Three independent layers. Each depends only on the manifest types. No layer imports from another layer.

```
┌────────────┐      ┌────────────┐
│  Validator  │─────▶│  Manifest  │◀─────┌────────────┐
│  (validate +│      │  (types)   │      │  Code Gen  │
│   estimate) │      └────────────┘      │  (planner + │
└────────────┘                           │   templates)│
                                         └────────────┘
```

The manifest is a plain object conforming to `types.ts`. It can be written by hand, produced by an LLM conversation, or generated by any tool. The code generator doesn't know or care which.

---

## Manifest Schema

The manifest is the contract. Everything downstream is mechanical. The TypeScript interfaces are the source of truth; YAML serialization is just for human readability and version control.

```typescript
interface Manifest {
  entities: Entity[];
  relationships: Relationship[];
  metric_clusters: MetricCluster[];
  bft_tables: BftTable[];
}

interface Entity {
  name: string;
  role: "leaf" | "bridge";
  detail: boolean;               // does this entity contribute rows?
  estimated_rows: number;
  metrics: MetricDef[];
}

interface MetricDef {
  name: string;
  type: "currency" | "integer" | "float" | "rating" | "percentage";
  nature: "additive" | "non-additive";
}

interface Relationship {
  name: string;
  between: [string, string];
  type: "many-to-many" | "many-to-one";
  estimated_links: number;
  weight_column?: string;        // e.g., "assignment_share"
}

interface MetricCluster {
  name: string;
  metrics: string[];             // references to MetricDef.name
  traversals: TraversalRule[];
}

interface TraversalRule {
  metric: string;
  on_foreign_rows: Strategy;
  weight?: string;               // for allocation: column name or "equal_split"
  weight_source?: string;        // relationship name or shared dimension
}

type Strategy = "reserve" | "elimination" | "allocation" | "sum_over_sum" | "direct";

interface BftTable {
  name: string;
  grain: string;                 // e.g., "Student × Class × Professor"
  grain_entities: string[];
  clusters_served: string[];
  estimated_rows: number;
  metrics: ResolvedMetric[];
  reserve_rows: string[];        // e.g., ["<Reserve Professor>"]
}

interface ResolvedMetric {
  metric: string;
  strategy: Strategy;
  weight?: string;
  weight_column?: string;        // output column name for sum/sum
  sum_safe: boolean;
  requires_reserve_rows: boolean;
}
```

---

## Cost Estimator

Row estimation is deterministic from Phase A cardinalities. The formulas live in `estimate.ts`:

```typescript
function estimateRows(
  entities: Entity[],
  relationships: Relationship[],
  grainEntities: string[]
): number;
```

Rules:
- Single entity with detail: `entity.estimated_rows`
- One M-M bridge: `relationship.estimated_links`
- Two M-M bridges sharing a bridge entity: `links₁ × (links₂ / bridge.estimated_rows)`
- Unrelated entities both with detail: sum of row counts (sparse union)
- Reserve rows: +1 per entity that has reserve-strategy metrics

The estimator also computes the fan-out multiplier per relationship, which Phase C uses to rank simplification impact.

---

## Code Generator

### Build Planner

`planner.ts` reads a complete manifest and produces an ordered list of build steps:

```typescript
interface BuildStep {
  order: number;
  filename: string;               // e.g., "01_base_enrollment.sql"
  description: string;
  depends_on: string[];           // filenames of prior steps
  type: "join" | "allocation" | "elimination" | "reserve" | "sum_over_sum" | "final";
}

function plan(manifest: Manifest): BuildStep[];
```

The ordering logic:
1. Base joins that establish the grain (one per BFT)
2. Strategy transformations (one per metric-strategy pair, parallelizable)
3. Final assembly join (combines all strategy outputs into the BFT)
4. Validation queries

### SQL Templates

Each strategy module exports a single function:

```typescript
// allocation.ts
function emitAllocation(metric: ResolvedMetric, table: BftTable, dialect: Dialect): string;

// elimination.ts
function emitElimination(metric: ResolvedMetric, table: BftTable, dialect: Dialect): string;

// reserve.ts
function emitReserve(metric: ResolvedMetric, table: BftTable, dialect: Dialect): string;

// sum-over-sum.ts
function emitSumOverSum(metric: ResolvedMetric, table: BftTable, dialect: Dialect): string;
```

Each returns a complete, executable SQL statement. No Jinja. No templating language. String interpolation from the manifest is the template engine.

```typescript
type Dialect = "duckdb" | "spark";
```

The dialect differences for the operations bft-maker uses (window functions, CTEs, aggregations, UNION ALL) are minimal. The dialect layer handles syntax variations like `CREATE OR REPLACE TABLE` vs `CREATE TABLE IF NOT EXISTS` and minor type casting differences.

### Validation Generator

`validation.ts` reads the manifest and emits one SQL query per assertion. Each query returns zero rows on success.

```typescript
function emitValidation(table: BftTable): ValidationQuery[];

interface ValidationQuery {
  name: string;                   // e.g., "allocation_sum_check_tuition_paid"
  description: string;
  sql: string;                    // returns rows only on failure
  severity: "error" | "warning"; // warnings for row count tolerance
}
```

### Output

`emit.ts` writes everything to a directory:

```
output/
├── 01_base_enrollment_join.sql
├── 02_allocate_tuition_paid.sql
├── 03_allocate_salary.sql
├── 04_eliminate_class_budget.sql
├── 05_reserve_overhead.sql
├── 06_sum_over_sum_satisfaction.sql
├── 07_final_assembly.sql
├── 08_validate.sql
├── run.sh
└── manifest.yaml                 # the manifest that produced this output
```

`run.sh` executes each file in order using DuckDB CLI:

```bash
#!/bin/bash
set -e
DB=${1:?"Usage: run.sh <database_path>"}
for f in [0-9]*.sql; do
  echo "Running $f..."
  duckdb "$DB" < "$f"
done
echo "All steps complete."
```

---

## CLI

The CLI reads a YAML manifest and writes output files:

```bash
npx bft-maker generate --manifest manifest.yaml --dialect duckdb --output ./sql/
npx bft-maker generate --manifest manifest.yaml --dialect spark --output ./sql/
npx bft-maker validate --manifest manifest.yaml
```

`validate` runs the manifest through the validator without generating code. `generate` validates first, then produces SQL.

---

## Testing Strategy

### Unit Tests

Pure function inputs and outputs. No SQL execution.

- `manifest/validate.ts`: feed it invalid manifests, assert it catches every inconsistency (missing relationship references, orphan metrics, impossible grains).
- `manifest/estimate.ts`: known cardinalities in, expected row counts out.
- `codegen/sql/*.ts`: snapshot tests. A fixture manifest produces expected SQL strings. When templates change, snapshots update.

### Integration Tests

A small set of end-to-end tests:

1. Feed the manifest to the code generator.
2. Execute the SQL against DuckDB with synthetic data.
3. Assert validation queries return zero rows.
4. Assert `SUM` of allocated metrics equals `SUM` of originals.

### Fixture Manifests

The `test/fixtures/` directory contains complete manifests for:

- **University**: Students, Classes, Professors (the running example from the spec)
- **Northwind**: Orders, Products, Employees with Northwind dataset
- **Simple**: Two unrelated entities sharing only time and org dimensions
- **Single entity**: Degenerate case — one entity, no foreign metrics, no strategies needed

Each fixture includes synthetic source data and expected output characteristics.

---

## What This Design Does Not Include

Things that are explicitly out of scope for v1:

- **A wizard UI.** The manifest is built through conversation (with an LLM or a colleague), not through a web form.
- **User authentication or multi-tenancy.** This is a local tool. Manifests are files.
- **A database for storing manifests.** The filesystem is the database. Git is the version history.
- **Source data profiling or introspection.** The user provides cardinalities. The engine trusts them.
- **BI tool integration.** The output is flat SQL tables. BI tools connect to them through their normal database connectors.
- **Incremental/streaming updates.** The generated SQL does full rebuilds.
- **Multiple SQL dialects beyond DuckDB and Spark SQL.** Two is enough. Others can be added later if needed.
